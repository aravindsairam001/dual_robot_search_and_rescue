Dual-Robotic System for Autonomous Search and Rescue

Author: Aravind Sairam Saravanan (2264341)
Supervisor: Dr. Hamidreza Nemati
Submitted: September 5, 2023
Degree: MSc in Robotics, University of Bristol & University of the West of England

‚∏ª

üìå Table of Contents
	‚Ä¢	Project Overview
	‚Ä¢	System Architecture
	‚Ä¢	Technologies Used
	‚Ä¢	Implementation Details
	‚Ä¢	Phase 1: Autonomous Mapping with the Husky Robot
	‚Ä¢	Phase 2: Human Detection & Rescue with a TurtleBot Swarm
	‚Ä¢	Performance Evaluation
	‚Ä¢	Husky Robot Metrics
	‚Ä¢	TurtleBot Swarm Metrics
	‚Ä¢	Project Setup Instructions
	‚Ä¢	Running the Simulation
	‚Ä¢	Conclusion & Future Recommendations
	‚Ä¢	License
	‚Ä¢	Acknowledgments

‚∏ª

üö® Project Overview

This project addresses the challenge of improving search and rescue operations in complex indoor environments (e.g., office buildings during emergencies) using a dual-robotic autonomous system.

The system is split into two phases:
	‚Ä¢	Husky Robot: Performs autonomous mapping of the unknown indoor space.
	‚Ä¢	TurtleBot Swarm: A group of 20 TurtleBots performs search and rescue operations using the generated map.

The system was simulated in a Gazebo environment and evaluated for localization, obstacle avoidance, and rescue efficiency.

‚∏ª

üß† System Architecture

‚úÖ Husky Robot (Primary Mapping Unit)
	‚Ä¢	Mobility: 4-wheel drive, rugged design
	‚Ä¢	Sensors: 3D LIDAR, Thermal Camera
	‚Ä¢	Navigation: Mapless (LIDAR-based SLAM)
	‚Ä¢	Software: move_base
	‚Ä¢	Payload: 75kg

‚úÖ TurtleBot Swarm (Search and Rescue Unit)
	‚Ä¢	Mobility: 2-wheel drive, agile
	‚Ä¢	Sensors: 2D LIDAR, RGB-D Camera
	‚Ä¢	Navigation: Map-based (AMCL + A*)
	‚Ä¢	Software: move_base, OpenCV DNN, YOLOv4-tiny
	‚Ä¢	Payload: 5kg

‚∏ª

üß∞ Technologies Used
	‚Ä¢	ROS Noetic ‚Äì Middleware and communication
	‚Ä¢	Gazebo ‚Äì Simulation environment
	‚Ä¢	RViz ‚Äì Visualization and debugging
	‚Ä¢	move_base ‚Äì Navigation stack
	‚Ä¢	Gmapping ‚Äì 2D SLAM for map generation
	‚Ä¢	AMCL ‚Äì Monte Carlo localization for TurtleBots
	‚Ä¢	A* ‚Äì Path planning algorithm
	‚Ä¢	OpenCV DNN ‚Äì Deep learning integration
	‚Ä¢	YOLOv4-tiny ‚Äì Pretrained object detection model

‚∏ª

üîß Implementation Details

Phase 1: Autonomous Mapping with the Husky Robot
	‚Ä¢	Configured Husky with no pre-loaded map.
	‚Ä¢	Used Gmapping to build 2D occupancy grid in real-time.
	‚Ä¢	Saved the generated map using:

rosrun map_server map_saver -f my_office_map



Phase 2: Human Detection & Rescue with a TurtleBot Swarm
	‚Ä¢	Deployed 20 TurtleBots into the mapped Gazebo world.
	‚Ä¢	Each TurtleBot:
	‚Ä¢	Used AMCL for localization.
	‚Ä¢	Navigated using A* algorithm.
	‚Ä¢	Processed RGB-D camera feed using YOLOv4-tiny with OpenCV DNN.
	‚Ä¢	Upon detecting a human:
	‚Ä¢	Cancelled current goal.
	‚Ä¢	Re-planned to nearest exit.
	‚Ä¢	Returned to home location.

‚∏ª

üìä Performance Evaluation

Husky Robot Metrics

Metric	Result	Notes
Avg. Localization Error	1.324 m	Slightly above goal of <1m
Collision Avoidance	72.7%	16/22 obstacles successfully avoided
Avg. Path Length	12.095 m	Measured across 3 runs
Navigation Efficiency	0.156 min/m¬≤	Time per square meter for 243m¬≤ area

TurtleBot Swarm Metrics

Metric	Result	Notes
Detection Accuracy	95%	Excellent object detection via YOLOv4-tiny
Guidance Accuracy	68.42%	Dynamic navigation still challenging
Return-to-Home Accuracy	61.54%	Final return stage was inconsistent
Stuck Rate	50%	Half the bots got stuck in tight corridors
Recovery Rate	90%	Custom recovery logic worked effectively


‚∏ª

üõ† Project Setup Instructions

# 1. Install ROS Noetic (Ubuntu 20.04)
# http://wiki.ros.org/noetic/Installation/Ubuntu

# 2. Create a Catkin workspace
mkdir -p ~/catkin_ws/src
cd ~/catkin_ws
catkin_make
source devel/setup.bash

# 3. Clone required repositories
cd ~/catkin_ws/src
git clone https://github.com/ros-planning/navigation.git
git clone https://github.com/leggedrobotics/darknet_ros.git
git clone https://github.com/clearpathrobotics/cpr_gazebo.git
# git clone <your project repo>

# 4. Install dependencies and build
cd ~/catkin_ws
rosdep install --from-paths src --ignore-src -r -y
catkin_make


‚∏ª

üöÄ Running the Simulation

# Step 1: Launch the office environment
roslaunch your_package_name office_environment.launch

# Step 2: Run Husky for mapping
roslaunch husky_navigation mapless_navigation.launch

# Step 3: Save the map
rosrun map_server map_saver -f office_map

# Step 4: Launch TurtleBot swarm for rescue
roslaunch turtlebot_swarm search_and_rescue.launch map_file:="/path/to/office_map.yaml"


‚∏ª

üßæ Conclusion & Future Recommendations

‚úÖ Accomplishments
	‚Ä¢	Built a full search and rescue simulation pipeline.
	‚Ä¢	Achieved high human detection success with YOLOv4-tiny.
	‚Ä¢	Demonstrated coordination between mapping and rescue robots.

üîß Recommendations
	‚Ä¢	Better Localization: Use sensor fusion (e.g., EKF) for Husky.
	‚Ä¢	Advanced Navigation: Integrate D* Lite or ML-based planners to reduce stuck rate.
	‚Ä¢	Hardware Testing: Extend project to real-world deployment using physical robots.

‚∏ª

üìù License

This project is licensed under the MIT License ‚Äì see the LICENSE.md file for details.

‚∏ª

üôè Acknowledgments
	‚Ä¢	Dr. Hamidreza Nemati (Supervisor)
	‚Ä¢	MSc Robotics Department, University of Bristol & UWE
	‚Ä¢	Friends and peers for motivation and testing support
