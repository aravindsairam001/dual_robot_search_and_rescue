My MSc Project: A Dual-Robotic System for Autonomous Search and RescueFor my MSc Robotics dissertation, I designed, built, and evaluated a comprehensive, multi-robotic framework for autonomous search and rescue missions. The entire system was developed within a simulated office environment to test its effectiveness in enhancing the safety and productivity of emergency response.This project was submitted in partial fulfillment of the requirements for the degree of Master of Science in Robotics at the University of Bristol and the University of the West of England.Author: Aravind Sairam Saravanan (2264341)Supervisor: Dr. Hamidreza NematiDate: September 5, 2023Table of ContentsProject OverviewSystem Architecture I DesignedTechnologies I UsedWhat I ImplementedPhase 1: Autonomous Mapping with the Husky RobotPhase 2: Human Detection & Rescue with a TurtleBot SwarmHow My System PerformedHusky Robot PerformanceTurtleBot Swarm PerformanceHow to Set Up the ProjectHow to Run My SimulationConclusion & My Recommendations for Future WorkLicenseAcknowledgmentsProject OverviewIn this project, I tackled the challenge of improving search and rescue operations in complex indoor spaces, like offices during a fire. My solution was a two-stage robotic system. First, I used a robust Husky robot to autonomously enter and map the unknown environment. Second, once the map was created, I deployed a swarm of 20 agile TurtleBots that I programmed to navigate the space, find stranded people using computer vision, and guide them to safety. I evaluated the entire system's performance based on key metrics like localization accuracy, obstacle avoidance, and the success rate of human detection and rescue tasks.System Architecture I DesignedI designed a framework that leverages the strengths of two different types of robots:Primary Mapping Unit (Husky Robot): I chose the Husky for its robustness and superior sensor capabilities. I configured it to be the initial explorer, responsible for creating a reliable map of the environment from scratch.Search and Rescue Swarm (20 TurtleBots): For the rescue phase, I implemented a swarm of 20 TurtleBots. Their small size and agility are perfect for navigating tight office corridors. I programmed them to work in parallel, using the map from the Husky to efficiently search for and rescue individuals.FeatureHusky RobotTurtleBotMy Assigned RoleInitial Mapping & ExplorationHuman Detection & GuidanceMobility4-wheel drive, Rugged2-wheel drive, AgilePrimary Sensors3D LIDAR, Thermal Camera2D LIDAR, RGB-D CameraNavigation I ImplementedMapless Navigation (initially)Map-based (A*, AMCL)Software I Integratedmove_basemove_base, OpenCV DNNPayload75kg5kgTechnologies I UsedTo build and test my system, I selected and integrated the following industry-standard tools:Robotic Operating System (ROS Noetic): This was the backbone of my project, which I used to manage all communication and processes between the robots and the simulation.Gazebo: I built the simulated office environment in Gazebo, placing the walls, obstacles, and human models to create a realistic testbed.RViz: I used RViz extensively to visualize the robots' sensor data, monitor the map creation process, and debug the navigation paths in real-time.move_base Package: I configured the move_base navigation stack for both the Husky and the TurtleBots to handle all path planning, obstacle avoidance, and localization tasks.AMCL (Adaptive Monte Carlo Localization): I used this package to enable the TurtleBots to accurately determine their position on the map I generated.A* Algorithm: I implemented the A* algorithm as the core path-planner for the TurtleBots, ensuring they could find the shortest safe path.OpenCV DNN Module & YOLOv4-tiny: For the critical task of human detection, I integrated the OpenCV DNN module into my ROS nodes and utilized a pre-trained YOLOv4-tiny model to process the camera feeds from the TurtleBots.What I ImplementedMy work was divided into two main, sequential phases.Phase 1: Autonomous Mapping with the Husky RobotMapless Navigation Setup: I configured the Husky's move_base parameters to operate without a pre-existing map, forcing it to rely entirely on its LIDAR sensor data.Dynamic Map Generation: I launched the configured Husky into my Gazebo world. As it navigated, the gmapping algorithm dynamically constructed a 2D occupancy grid map of the office.Map Saving: Once the Husky had explored the entire area, I used the map_server utility to save the completed map, making it available for the next phase.rosrun map_server map_saver -f my_office_map
Phase 2: Human Detection & Rescue with a TurtleBot SwarmSwarm Deployment: I wrote launch files to deploy a swarm of 20 TurtleBots into the simulation, providing each one with the map generated by the Husky.Localization and Navigation: I configured each TurtleBot to use the AMCL package for localization and the A* algorithm for path planning, enabling them to navigate to predefined search waypoints.Computer Vision Integration: This was a key part of my work. I wrote a ROS node that subscribed to each TurtleBot's camera feed. This node used the OpenCV DNN module to run the YOLOv4-tiny model, allowing it to detect humans in real-time.Rescue Logic: I developed the logic that, upon detecting a human, would make the TurtleBot cancel its current goal, calculate a new path to the nearest safe exit, and then proceed to its home position, completing the rescue cycle.How My System PerformedI ran the full simulation three times to gather quantitative data on my system's effectiveness.Husky Robot PerformanceMetricResultNotesAvg. Localization Error1.324 mSlightly above my initial target of <1m.Collision Avoidance Ratio72.7%Successfully avoided 16 out of 22 obstacles.Avg. Path Length12.095 mMeasured across three runs to a target destination.Navigation Efficiency0.156 minutes/m²The time it took my system to map the 243m² area.TurtleBot Swarm PerformanceMetricResultNotesDetection Accuracy95%The YOLOv4 integration was highly successful.Guidance Accuracy68.42%Some bots struggled with dynamic path changes after detection.Return-to-Home Accuracy61.54%The final navigation stage proved challenging for some bots.Stuck Rate50%Half the bots got stuck at some point, often in tight corners.Recovery Rate90%My recovery behaviors were effective for most stuck bots.How to Set Up the ProjectTo replicate my work, you would need to:Install ROS Noetic: Follow the official ROS installation guide.Create a Catkin Workspace:mkdir -p ~/catkin_ws/src
cd ~/catkin_ws/
catkin_make
source devel/setup.bash
Clone My Repositories and Dependencies: Clone my project repository and the required dependencies into your src directory.cd ~/catkin_ws/src
# git clone <your_repo_url>
git clone https://github.com/ros-planning/navigation.git
git clone https://github.com/leggedrobotics/darknet_ros.git
git clone https://github.com/clearpathrobotics/cpr_gazebo.git
# Add other necessary repositories for Husky and TurtleBot simulations
Install Dependencies & Build:cd ~/catkin_ws
rosdep install --from-paths src --ignore-src -r -y
catkin_make
How to Run My Simulation(This is a conceptual guide. The actual launch files are in the repository.)Launch my Gazebo Environment:roslaunch your_package_name office_environment.launch
Run the Husky Mapping Phase:roslaunch husky_navigation mapless_navigation.launch
Save the Map:rosrun map_server map_saver -f office_map
Run the TurtleBot Search and Rescue Phase:roslaunch turtlebot_swarm search_and_rescue.launch map_file:="/path/to/office_map.yaml"
Conclusion & My Recommendations for Future WorkThrough this project, I successfully built and demonstrated a functional dual-robotic system for autonomous search and rescue. The high success rate of my human detection implementation was a major achievement. Based on my results, I have the following recommendations for anyone looking to build upon my work:Improve Localization: Future work should focus on implementing more advanced sensor fusion techniques, like a Kalman filter, to reduce the Husky's localization error.Smarter Navigation: To reduce the Stuck Rate, I suggest integrating machine learning-based navigation algorithms or more dynamic planners like D* Lite.Real-World Testing: The next logical step is to move beyond simulation and test these algorithms on physical robots to see how they perform with real-world sensor noise and environmental uncertainties.LicenseThis project is licensed under the MIT License - see the LICENSE.md file for details.AcknowledgmentsI want to extend my sincere gratitude to my supervisor, Dr. Hamidreza Nemati, for his invaluable guidance and support throughout this project.Thank you to the MSc Robotics department for providing a conducive research environment.Finally, a huge thanks to my friends and colleagues for their constant motivation and encouragement.
